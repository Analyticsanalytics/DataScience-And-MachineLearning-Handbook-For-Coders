## 词与文档的表示

表示学习是机器学习的重要组成部分，在计算机视觉、图像处理等多个领域都发挥着重要的作用。所谓词与文档的表示，即是将自然语言转化为形式化或者数学的表示，以方便计算机理解与进行处理。而人类的语言因为其多变的语法结构与丰富的语义信息使其充满了歧义性，必须结合一定的上下文才能产生正确的理解与表示。另一方面，因为自然语言往往具有嵌套复杂的层次结构，譬如其语言粒度可能包含文档、段落、句子、短语、词等多个不同的部分；在词与文档的表示中我们同样需要考虑这些不同粒度的语法单元，为其选择合适的表示数据结构。在谈论词与文档的表示模型时，我们经常会用稀疏（Sparse）与密集（Dense），离散（Discrete）与连续（Continuous）这些形容词。稀疏代指某个特征向量中绝大部分是零值，而密集即特征向量中绝大部分是非零值。稀疏向量往往具有较好的可解释性，毕竟因为仅有几位有值，人们可以更好地通过非零下标对应的词信息来推导出向量的表示语义；而连续型密集向量则对于人类而言较难理解。离散表示则是将词或文档看做离散的符号，又称为局部表示，其缺点是忽略了文本中词的先后顺序以及词的具体语义，并且不能较好地衡量词与词之间的距离。而连续表示即是将词或文档以连续向量的形式表示为连续空间中的某个点，从而允许在下游应用中以余弦距离或者欧式距离的方式来计算词或文档之间的相似度，能够有效地解决词的歧义问题。
常见的连续表示包含了分布式表示（Distributional Representation）与分散式表示（Distributed Representation）这两种。分布式表示以 Harris 的分布式假设为基础，即两个词的相似度受到两个词的上下文相似度的影响；这里的上下文可以是相邻词、滑动窗口内的词以及所在文档的词等多个维度，最简单的分布式表示就是以词与上下文的共现矩阵中的行向量作为词的表示向量。而分散式表示则依托于深度学习与神经网络，侧重于将语义分散存储。典型的分散式表示就是词嵌入（Word Embeddings），其将词或文档的语义与语法特征分散式地存储于一组神经元中，并且最终生成连续密集型向量作为词表示，其能够让特征表示与模型更加紧凑。需要强调的是这二者相辅相成，对于我们熟知的 Word2Vec 中的 CBOW 或 Skip-gram 模型、Glove 模型等得到的词向量而言，其兼具了分散式表示与分布式表示的特点，并非非此即彼的排斥关系，我们在其他段落中也统一使用分布式表示进行表述。

### 离散表示

早期人们习惯用稀疏离散的向量来表示词或文档，即将符号或者符号序列转化为维度较高的稀疏向量。典型的离散表示就是独热码（One-Hot），即某个词的表示向量中仅有该词对应的词表中下标位为 1，其余位为 0；然后可以通过词袋模型（Bag-of-Words, BOW）或者 N-gram 或者 TF-IDF 等方式转化为文档的表示。譬如我们的语料库中包含两条语句：John likes to watch movies. Mary likes too. 以及 John also likes to watch football| games. 那么对整个语料库进行分词处理之后，我们能够得出如下词表：
| John | likes | to   | watch | movies | also | football | games | Mary | too  |
| ---- | ----- | ---- | ----- | ------ | ---- | -------- | ----- | ---- | ---- |
| 0    | 1     | 2    | 3     | 4      | 5    | 6        | 7     | 8    | 9    |
分析上表可以发现词表中共有 10 个词，并且我们为每个词分配了唯一的索引；需要主题的是，词在词表中的索引与其在语料集中出现的顺序并没有必然关联。基于如上词索引，我们能够推导出词 John 的表示为$[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]$。在得到词表示之后，我们可以运算将其推导为整个文档的表示。词袋模型 BOW 中，我们直接将各词的词向量表示加和作为整个文档的表示，譬如此时语料集中文档的表示向量就分别为$[1,2,1,1,1,0,0,0,1,1]$与$[1,1,1,1,0,1,1,1,0,0]$。不过词袋模型往往受限于无法识别常用词与专用词，我们可以使用逆文档词频 TF-IDF（Term Frequency - Inverse Document Frequency）来优化文档的向量表示。逆文档词频的计算公式为$log(1 + \frac{N}{n_t})$，其中$N$表示文档总数而$n_t$表示含有词$t$的文档数。根据 TF-IDF 计算得到的文档的向量表示为$[0.693, 1.386, 0.693, 0.693, 1.099, 0, 0, 0, 0.693, 0.693]$。不过这两种模型没有考虑到词的顺序信息，我们可以引入统计语言模型中 N-gram 模型来对词序列而不仅仅是单个词进行建模：
| John likes | likes to | to watch  | watch movies | Mary likes | likes too | John also | also likes | watch football | football games |
| ---- | ----- | ---- | ----- | ------ | ---- | -------- | ----- | ---- | ---- |
| 0    | 1     | 2    | 3     | 4      | 5    | 6        | 7     | 8    | 9    |
分析上表可以发现我们的原子词表不再是包含了单个词，而是包含了两个词的序列，文档的表示也就对应地变为了$[1, 1, 1, 1, 1, 1, 0,0,0, 0]$与$[0,1,1,0,0,0,1,1,1,1]$。不过这种模型虽然引入了词的顺序，但是词表会随着语料集的增加而迅速膨胀。对于词表长度为$n$的 Uni-gram 模型，模型参数数量为$2 \times 10^5$，Bi-gram 的模型参数数量为$4 \times 10^{10}$，Tri-gram 的模型参数数量为$8 \times 10^{15}$，4-gram 的模型参数数量为$16 \times 10^{20}$，可以看出在 N-gram 模型中随着上下文窗口维度的增加模型参数数量以几何级数增长。
BOW 或者 [Harris, 1954] 中提出的 bag-of-n-grams 同样可以将文本转化为固定长度的特征向量，不过其缺陷在于无法较好地提现向量不同维度之间的语义、语法关联性；尽管 bag-of-n-grams 这样的模型也考虑了局部上下文中的词顺序因素，但是仍然会丢失掉所谓局部上下文的信息。并且在词表较大的情况下，基于词袋模型构造出的文档特征向量中大部分位取零，整个特征向量是典型的稀疏向量，其维度会随着词表的增加而迅速扩张。另一方面，这些模型都不能较好地度量词的语义以及定量地反映出词之间的距离关系。譬如`powerful`,`strong`以及`Paris`这些词虽然前二者语义相近，与后者语义上相差甚远，但是其在词袋模型中的相对距离却是一致的。


### 连续分布式表示与主题模型

分布式表示（Distributional Representation）源于 [Harris, 1954] 的分布式假设，即可以用某个词附近的其他词来表示该词，这样如果两个词具有相似的上下文，那么我们可以认为这两个词也是类似的。在此基础上基于数据驱动的能够捕获词法属性与语义属性的词的向量表示成为现代自然语言处理领域重要的技术手段之一。最简单的分布式表示模型就是基于共现矩阵（Concurrence Matrix），假设有维度为$W \times C$的共现矩阵$M$，其中$W$为词表大小，$C$是上下文窗口长度。如果我们的语料集包含三个文档：I like deep learning, I like NLP, I enjoy ﬂying，那么该语料集对应的词表中的共现矩阵可以表示为：

| word     | I    | like | enjoy | deep | learning | NLP  | flying |
| -------- | ---- | ---- | ----- | ---- | -------- | ---- | ------ |
| I        | 0    | 2    | 1     | 0    | 0        | 0    | 0      |
| like     | 2    | 0    | 0     | 1    | 0        | 1    | 0      |
| enjoy    | 1    | 0    | 0     | 0    | 0        | 0    | 1      |
| deep     | 0    | 1    | 0     | 0    | 1        | 0    | 0      |
| learning | 0    | 0    | 0     | 1    | 0        | 0    | 0      |
| NLP      | 0    | 1    | 0     | 0    | 0        | 0    | 0      |
| flying   | 0    | 0    | 1     | 0    | 0        | 0    | 0      |

我们可以选择共现矩阵中的某一行作为词的特征向量，不过这种简单的共现矩阵模型其向量维度同样会随着词表的增长而线性增长，存储整个词表的空间消耗非常大，并且对于下游应用而言存在稀疏性问题，模型相对不太稳定。我们可以通过简单的 SVD 降维来优化共现矩阵，其他的分布式表示模型还包括潜在语义分析（Latent Semantic Analysis，LSA）、潜在狄利克雷分配（Latent Dirichlet Allocation，LDA）以及随机索引（Random Indexing）等等。基于概率统计的 pLSA（Probabilistic Latent Semantic Analysis） 概率隐语义分析模型，增加了主题模型，形成简单的贝叶斯网络，可以使用 EM 算法学习模型参数。

![](https://coding.net/u/hoteam/p/Cache/git/raw/master/2017/3/2/401B5F00-7A24-42D6-9396-D7484AC4AA4D.png)

其中$D$代表文档，$Z$代表主题(隐含类别)，$W$表示词：
- $P(d_i)$表示文档$d_i$的出现的概率
- $P(z_k|d_i)$表示文档$d_i$中$z_k$出现的概率
- $P(w_j|z_k)$表示给定主题$z_k$的前提下词$w_j$出现的概率

每个文档在所有主题上服从多项分布，每个主题在所有词项上服从多项分布。文档譬如有爱情有武侠等多个主题，那么服从每个主题的概率是不定的。那么，整个文档或者语料库的生成过程是这样的，首先以$P(d_i)$的概率选中文档$d_i$，然后以$P(z_k|d_i)$的概率选中主题$z_k$，最后以$P(w_j|z_k)$的概率产生一个词$w_j$。一般来说，主题会是一个隐藏语义，那么在主题预测中，即观察数据为$(d_i,w_j)$对时，主题$z_k$是隐含变量，而$(d_i,w_j)$的联合分布为：
$$
\begin{equation}
P(d_i,w_j)=P(w_j|d_i)P(d_i) \\
P(w_j|d_i)=\sum_{k=1}^{K}P(w_j|z_k)P(z_k|d_i)
\end{equation}
$$
而$P(w_j|z_k)$，$P(z_k|d_i)$对应了两组多项分布，即该模型的任务目标就是计算每个文档中的主题分布。我们使用极大似然估计来估计这个隐藏分布值，极大似然估计的目标函数即是希望$P(d_i,w_j)$的连乘积最大，即：
$$
L=\prod_{i=1}^{N}\prod_{j=1}^{M}P(d_i,w_j)=\prod_i\prod_jP(d_i,w_j)^{n(d_i,w_j)}
$$
将目标函数转化为 log 对数，即：
$$
l = \sum_i\sum_j n(d_i,w_j)log\left( \sum_{k=1}^{K}P(w_j|z_k)P(z_k|d_i)P(d_i) \right)
$$
其中$n(d_i,w_j)$为$w_j$在$d_i$中出现的次数。那么求整个隐含变量主题$z_k$的过程主要分为两步，E 步骤的结果为：
$$
P(z_k | d_i,w_j)=\frac{P(w_j|z_k)P(z_k|d_i)}{\sum_{l=1}^K P(w_j | z_l)P(z_l |d_i)}
$$
M 步骤的目标方程为：
$$
\left \{ 
\begin{aligned}
P(w_j | z_k) = \frac{\sum_i n(d_i,w_j)P(z_k|d_i,w_j)}{\sum_{m=1}^{M}\sum_i{n(d_i,w_j)P(z_k|d_i,w_j)}} \\ P(z_k | d_i) = \frac{\sum_j n(d_i,w_j)P(z_k|d_i,w_j)}{\sum_{k=1}^{K}\sum_j{n(d_i,w_j)P(z_k|d_i,w_j)}} \end{aligned} \right.
$$
pLSA 应用于信息检索，过滤、自然语言处理等领域，pLSA 考虑到词分布和主题分布，使用 EM 算法来学习参数，可以看做概率化的矩阵分解。pLSA 不需要先验概率即完成自学习，这是它的优势，但是在某些情况下还是需要有先验知识的影响。


### 连续分散式表示与词嵌入

分散式表示（Distributed Representation）能够以固定维度的密集连续向量来表示词或文档，该特征向量能够较好地反映词之间的语义与语法关联度。而词的分布式表示又称为词嵌入（Word Embedding），其能够以高效地方式将词表示为低维嵌入空间中的连续特征向量 （[Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015]）。这些嵌入式词特征向量能够应用于标识语义相近的词对（[ Turney, 2006; Agirre et al., 2009 ]）或者提取下游文本处理应用中的特征（[Turian et al., 2010; Guo et al., 2014]）。词嵌入模型中的核心组件是所谓的嵌入式链接函数，其接受嵌入作为参数根据上下文词预测某个词的分布。目前也存在很多的词向量构造模型，[Deerwester et al., 1990] 中使用底层的共现词的统计数据来构成向量，而 [Collobert et al., 2008] 则使用神经网络模型来从词序列中提取出内建表示作为特征向量。目前最流行的构建词的连续特征向量的模型算是 [Mikolov et al., 2013] 中提出的 Word2Vec 模型，它能够从无标注的语料集中生成 Skip-gram 模型的嵌入式向量。该模型中，会将目标词作为输入传递到某个包含连续投影层的对数线性分类器中，通过计算其上下文词出现的概率来计算误差，并且反向传导到投影层中完成参数估计。不过这几种词向量技术就好像其他很多深度学习实现一样仍然不透明（Opaque），即我们并不能用严格的数学推导去论证其原理；而 Glove 则提出了清晰的词向量模型及其推导过程，并且提出了结合了全局矩阵分解（Global Matrix Factorization）与本地滑动窗口（Local Context Window）方法优势的双线性对数拟合模型（Global Logbilinear Regression Model）。[Jeffrey Pennington, 2014] 中提出 Glove 能够基于非零的词-词协同出现频次的矩阵，而不是整个词-文档的稀疏矩阵或者某个大型语料集中的部分独立上下文窗口。
既然词嵌入方法在词的表示上已经取得了不俗的效果，那么自然也是可以应用到文档表示上，我们希望能够找到合适的方法将文档中整体的语义编码到固定维度的向量中。最简单直观的方式就是神经网络词袋模型，我们可以假设已知文档中所有词的嵌入向量，然后将文档视作 Bag-of-Vectors。文档中相似的词自然会指向相同的方向，形成语义聚类；而语义聚类的中心向量（Semantic Centroids）可以代表着该聚类中词的通用语义，我们可以使用这些语义中心向量以及词的数目来表示该文档。再扩展一点来看，同一领域的语料集中的文档可能会具有相似的语义聚类。通过学习不同文档之间具有的相似的模式，我们可以推导出类似于主题这样的噪音更少的通用语义中心向量。[Le et al., 2014] 提出了 Paragraph Vector 模型，它假设每个文档片都包含一个段落隐变量，就好像词的隐变量一样会影响到文档中的所有词。Paragraph Vector 有效解决了 BOW 模型中的局部上下文忽略与语义丢失的问题，可以看做 Word2Vec 在文本方面的扩展。其他的方法还包括递归神经网络（Recursive Neural Network），其按照成分句法树这样的预设的外部拓扑结构来递归推导出文档表示向量；也可以使用循环神经网络（Recurrent Neural Network），将文档看做类似于时间的序列；还可以使用卷积神经网络（Convolutional Neural Network），通过多个混合排布的卷积与采用层来生成文档的固定向量表示。基于以上描述的多种基础模型，研究者也提出了双向循环神经网络（Bi-directional Recurrent Neural Network）、长短时记忆模型（Long-Short Term Memory）等综合模型。