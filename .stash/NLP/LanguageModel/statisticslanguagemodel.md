<!-- toc -->

传统的统计语言模型是表示语言基本单位（一般为句子）的概率分布函数， 这个概率分布也就是该语言的生成模型。一般语言模型可以使用各个词语条件概率的形式表示：

$$p(s)=p(w_1^T)=p(w_1,w_2,\dots,w_T)=\Pi^T_{t=1}p(w_t|Context) $$

目标也可以是采用极大似然估计来求取最大化的Log概率的平均值，公式为$$\frac{1}{T}\sum^T_{t=1}\sum_{-c \le j\le c,j \ne0}log p(w_{t+j}|w_t)$$。

其中：

- $$c$$是训练上下文的大小。譬如$$c$$取值为5的情况下，一次就拿5个连续的词语进行训练。一般来说$$c$$越大，效果越好，但是花费的时间也会越多。
- $$p(w_{t+j}|w_t)$$表示$$w_t$$条件下出现$$w_{t+j}$$的概率。

其中Context即为上下文，根据对Context不同的划分方法，可以分为五大类。

### 上下文无关模型（Context=NULL)

该模型仅仅考虑当前词本身的概率，不考虑该词所对应的上下文环境。这是一种最简单，易于实现，但没有多大实际应用价值的统计语言模型。

$$p(w_t|Context)=p(w_t)=\frac{N_{w_t}}{N}$$

这个模型不考虑任何上下文信息，仅仅依赖于训练文本中的词频统计。它是 n-gram模型中当n=1的特殊情形，所以有时也称作Unigram Model (—元文法统计模型）。实际应用中，常被应用到一些商用语音识别系统中。

### N-Gram模型(Context=$$w_{t-n+1},w_{t-n+2},\dots,w_{t-1}$$)

N-Gram模型时大词汇连续语音识别中常用的一种语言模型，对中文而言，我们称之为汉语语言模型（CLM, Chinese Language Model）。汉语语言模型利用上下文中相邻词间的搭配信息，在需要把连续无空格的拼音、笔画，或代表字母或笔画的数字，转换成汉字串（即句子）时，可以计算出最大概率的句子，从而实现从到汉字的自动转换，无需用户手动选择，避开了许多汉字对应一个相同的拼音（或笔画串、数字串）的重码问题。该模型基于这样一种假设，第n个词的出现只与前面n-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现的概率的乘积。这些概率可以通过直接从语料中统计n个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram。

n=1时，就是上面所说的上下文无关模型，这里N-Gram—般认为是$$N\ge2$$是 的上下文相关模型。当n=2时，也称为Bigram语言模型，直观的想，在自然语 言中“白色汽车”的概率比“白色飞翔”的概率要大很多，也就是P(汽车|白色)> P(飞翔|白色)。n>2也类似，只是往前看n-1个词而不是一个词。一般N-Gram模型优化的目标是最大log似然，即：

$$\Pi^T_{t=1}p_t(w_t|w_{t-n+1},w_t|w_{t-n+2},\dots,w_t|w_{t-1})$$

N-Gram模型的优点包含了前N-1个词所能提供的全部信息，这些信息对当前 词出现具有很强的约束力。同时因为只看N-1个词而不是所有词也使得模型的效率较高。这里以Bi-Gram做一个实例，假设语料库总的词数为13748：

![image](http://images.cnitblog.com/blog/407700/201310/18171638-c87b895734e748ff9a188265bccbe6bb.png)

![](http://images.cnitblog.com/blog/407700/201310/18171638-c325ffe1717e4763838913964e3971fc.png)

N-Gram语言模型也存在一些问题：

- n-gram语言模型无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型。大部分研究或工作都是使用Trigram，就算使用高阶的模型，其统计 到的概率可信度就大打折扣，还有一些比较小的问题采用Bigram。


- 这种模型无法建模出词之间的相似度，有时候两个具有某种相似性的词，如果一个词经常出现在某段词之后，那么也许另一个词出现在这段词后面的概率也比较大。比如“白色的汽车”经常出现，那完全可以认为“白色的轿车”也可能经常出现。


- 训练语料里面有些n元组没有出现过，其对应的条件概率就是0,导致计算一整句话的概率为0。

#### 平滑法

方法一为平滑法。最简单的方法是把每个n元组的出现次数加1，那么原来出现k次的某个n元组就会记为k+1次，原来出现0次的n元组就会记为出现1 次。这种也称为Laplace平滑。当然还有很多更复杂的其他平滑方法，其本质都 是将模型变为贝叶斯模型，通过引入先验分布打破似然一统天下的局面。而引入 先验方法的不同也就产生了很多不同的平滑方法。

#### 回退法

方法二是回退法。有点像决策树中的后剪枝方法，即如果n元的概率不到， 那就往上回退一步，用n-1元的概率乘上一个权重来模拟。

### N-Pos模型(Context = $$c(w_{t-n+1}),c(w_{t-n+2}),\dots,c(w_{t-1})$$)

严格来说N-Pos只是N-Gram的一种衍生模型。N-Gram模型假定第t个词出现概率条件依赖它前N-1个词，而现实中很多词出现的概率是条件依赖于它前面词的语法功能的。N-Pos模型就是基于这种假设的模型，它将词按照其语法功能进行分类，由这些词类决定下一个词出现的概率。这样的词类称为词性 (Part-of-Speech，简称为POS)。N-Pos模型中的每个词的条件概率表示为：

$$p(s)=p(w^T_1)=p(w_1,w_2,\dots,w_T)= \\ \Pi^T_{t=1}p(w_t|c(w_{t-n+1}),c(w_{t-n+2}),\dots,c(w_{t-1}))$$

$$c$$为类别映射函数，即把$$T$$个词映射到$$K$$个类别($$1 \le K \le T$$)，实际上N-Pos使用了一种聚类的思想，使得N-Gram中$$w_{t-n+1},w_{t-n+2},\dots,w_{t-1}$$中的可能为$$T^{n-1}$$减少为$$c(w_{t-n+1}),c(w_{t-n+2}),\dots,c(w_{t-1})$$中的$$K^{N-1}$$，同时这种减少还采用了语义有意义的类别。

### 基于决策树的语言模型

上面提到的上下文无关语言模型、n-gram语言模型、n-pos语言模型等等，都可以以统计决策树的形式表示出来。而统计决策树中每个结点的决策规则是一 个上下文相关的问题。这些问题可以是“前一个词时w吗？ ”“前一个词属于类别C,吗？”。当然基于决策树的语言模型还可以更灵活一些，可以是一些“前一个词是动词?”，“后面有介词吗?”之类的复杂语法语义问题。基于决策树的语言模型优点是：分布数不是预先固定好的，而是根据训练预 料库中的实际情况确定，更为灵活。缺点是：构造统计决策树的问题很困难，且时空开销很大。

### 最大熵模型

最大熵原理是E.T. Jayness于上世纪50年代提出的，其基本思想是：对一个 随机事件的概率分布进行预测时，在满足全部已知的条件下对未知的情况不做任何主观假设。从信息论的角度来说就是：在只掌握关于未知分布的部分知识时,应当选取符合这些知识但又能使得熵最大的概率分布。

$$p(w|Context)=\frac{e^{\Sigma_i \lambda_i f_i(context,w)}}{Z(Context)}$$

### 自适应语言模型

前面的模型概率分布都是预先从训练语料库中估算好的，属于静态语言模型。 而自适应语言模型类似是Online Learning的过程，即根据少量新数据动态调整模型，属于动态模型。在自然语言中，经常出现这样现象：某些在文本中通常很少出现的词，在某一局部文本中突然大量地出现。能够根据词在局部文本中出现的 情况动态地调整语言模型中的概率分布数据的语言模型成为动态、自适应或者基于缓存的语言模型。通常的做法是将静态模型与动态模型通过参数融合到一起， 这种混合模型可以有效地避免数据稀疏的问题。还有一种主题相关的自适应语言模型，直观的例子为：专门针对体育相关内 容训练一个语言模型，同时保留所有语料训练的整体语言模型，当新来的数据属 于体育类别时，其应该使用的模型就是体育相关主题模型和整体语言模型相融合 的混合模型。

### Skip-Gram

> [A CloserLook at Skip-gram Modelling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)

根据论文中的定义可知道，常说的`k-skip-n-grams`在句子$$w_1 \dots w_m$$可以表示为：

$$\{ w_{i_1},w_{i_2}, \dots w_{i_n} | \sum_{j=1}^{n}i_j - i_{j-1} < k \}$$

Skip-gram 實際上的定義很簡單，就是允许跳几个字的意思… 依照原論文裡的定義，這個句子：

> Insurgents killed in ongoing fighting.

​    在 bi-grams 的時候是拆成：{    `insurgents killed, killed in, in ongoing, ongoing fighting`    }。  

​    在 2-skip-bi-grams 的時候拆成：{    `insurgents killed, insurgents in, insurgents ongoing, killed in, killed ongoing, killed fighting, in ongoing, in fighting, ongoing fighting`    }。  

​    在 tri-grams 的時候是：{    `insurgents killed in, killed in ongoing, in ongoing fighting`    }。  

​    在 2-skip-tri-grams 的時候是：{    `insurgents killed in, insurgents killed ongoing, insurgents killed fighting, insurgentsin ongoing, insurgents in fighting, insurgents ongoing fighting, killed in ongoing, killed in fighting, killed ongoing fighting, in ongoing fighting`    }。  

对于上文的语言模型的目标公式而言，Skip-Gram模型中的$$p(w_{t+j} | w_t)$$公式采用的是Softmax函数：

$$p(w_o | w_I) = \frac{exp(v'^T_{w_o}v_{w_I})}{\sum^W_{w=1}exp(v'^T_wv_{w_I})}$$



其中$$p(w_o | w_I)$$表示在词语$$w_I$$条件下出现$$w_o$$的概率，$$v_{w_o}$$表示$$w_o$$代表的词向量，而$$v_w$$代表词汇表中所有词语的向量。$$W$$是词汇表的长度。不过该公式不太切实际，因为$$W$$太大了，通常是$$10^5–10^7$$。

#### Hierarchical Softmax

这种是原始skip-gram模型的变形。我们假设有这么一棵二叉树，每个叶子节点对应词汇表的词语，一一对应。所以我们可以通过这棵树来找到一条路径来找到某个词语。比如我们可以对词汇表，根据词频，建立一棵huffman树。每个词语都会对应一个huffman编码，huffman编码就反映了这个词语在huffman树的路径。对于每个节点，都会定义孩子节点概率，左节点跟右节点的概率不同的，具体跟输入有关。譬如，待训练的词组中存在一句：“我爱中国”。

输入：爱

预测：我

假设，“我”的Huffman编码是1101，那么就在Huffman树上从根节点沿着往下走，每次走的时候，我们会根据当前节点和“爱”的向量算出（具体怎么算先不管），走到下一个节点的概率是多少。于是，我们得到一连串的概率，我们的目标就是使得这些概率的连乘值（联合概率）最大。

$$p(w|w_I)=\Pi_{j=1}^{L(w)-1}\sigma([n(w,j+1)=ch(n(w,j))]*v'^T_{n(w,j)}v_{w_I})$$

- $$L(w)$$为词语$$w$$在二叉树路径中的长度
- $$\sigma(*)$$即为Sigmoid函数
- $$n(w,j+1)$$即指$$w$$在二叉树的第$$j+1$$个节点
- $$ch(n(w,j))$$表示定义了任意一个固定的节点，要么是左，要么是右。合起来的意思是左右节点的正负号是不一致的，可以是左负右正，可以是左正右负。

而对于单一的选择左右节点的概率：

$$\sigma(x)=\frac{1}{1+e^{-x}} \\ \sigma(-x)=\frac{1}{1+e^{x}} \\ \sigma(x) + \sigma(-x) = 1 $$

显然，我们计算这个联合概率的复杂度取决了词语在huffman树的路径长度，显然她比W小得多了。另外，由于按词频建立的huffman树，词频高的，huffman编码短，计算起来就比较快。词频高的需要计算概率的次数肯定多，而huffman让高频词计算概率的速度比低频词的快。这也是很犀利的一个设计。

## NNLM

NNLM是Neural Network Language Model的缩写，即神经网络语言模型。神经网络语言模型方面最值得阅读的文章是Deep Learning二号任人物Bengio的《A Neural Probabilistic Language Model》，JMLR 2003。NNLM米用的是Distributed Representation，即每个词被表示为一个浮点向量。其模型图如下：

![](http://7xlgth.com1.z0.glb.clouddn.com/2288BF90-FD22-493A-B703-C5AB32726FF2.png)

目标是要学到一个好的模型：

$$f(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1})=p(w_t|w_1^{t-1})$$

需要满足的约束为：

$$f(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1}) > 0$$

$$\Sigma_{i=1}^{|V|} f(i,w_{t-1},\dots,w_{t-n+2},w_{t-n+1}) = 1$$

上图中，每个是输入词都被映射为一个向量，该映射用$$C$$表示，所以$$C(w_{t-1})$$即为$$w_{t-1}$$的词向量。$$g$$为一个前馈或者递归神级网络，其输出是一个向量，向量中的第$$i$$个元素表示概率$$p(w_t=i|w_1^{t-1})$$。训练的目标依然是最大似然加正则项，即：

$$Max Likelihood = max \frac{1}{T}\sum_tlogf(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1};\theta) \\ + R(\theta)$$

其中$$\theta$$为参数，$$R(\theta)$$为正则项，输出层采用sofamax函数：

$$p(w_t|w_{t-1},\dots,w_{t-n+2},w_{t-n+1})=\frac{e^{y_{w_t}}}{\sum_ie^{y_i}}$$

其中$$y_i$$是每个输出词$$i$$的未归一化$$log$$概率，计算如下：

$$y=b+Wx+Utanh(d+Hx)$$

其中$$b,W,U,d,H$$都是参数，$$x$$为输入，需要注意的是，一般的神级网络输入是不需要优化，而在这里，$$x=(C(w_{t-1}),C(w_{t-2}),\dots,C(w_{t-n+1}))$$，也是需要优化的参数。在图中，如果下层原始输入$$x$$不直接连到输出的话，可以令$$b=0$$，$$W=0$$。如果采用随机梯度算法的话，梯度的更新规则为：

$$ \theta + \epsilon \frac{\partial log p(w_t | w_{t-1},\dots,w_{t-n+2},w_{t-n+1})}{\partial \theta} \to \theta $$



其中$$\epsilon$$为学习速率，需要注意的是，一般神级网络的输入层只是一个输入值，而在这里，输入层$$x$$也是参数(存在$$C$$中)，也是需要优化的。优化结束之后，词向量有了，语言模型也有了。这个Softmax模型使得概率取值为(0,1)，因此不会出现概率为0的情况，也就是自带平滑，无需传统N-Gram模型中那些复杂的平滑算法。Bengio在APNews数据集上做的对比实验也表明他的模型效果比精心设计平滑算法的普 通N-Gram算法要好10%到20%。
